<h1>Word2Vec: A gentle Introduction</h1>
  
It is a word embedding technique used particularly while handling text data for NLP related tasks.

`word embedding` :Representing text in form of real valued vector such that words that are closer in vector space are expected to be similar in meaning.

Introduced in 2013 by `Google` ( <a href="https://arxiv.org/pdf/1301.3781.pdf
"> View research Paper</a> )

Key features of word2vec:

1.Semantic meaning 

2.Low dimension(generally 200-300 dimension) as compared to One Hot encoding,bag of words,ngram or Tf-Idf etc.

3.We will get a dense vector(so overfitting problem will be reduced)

Special Thanks to @github/campusx-official for <a href="https://www.youtube.com/watch?v=DDfLc5AHoJI">This video</a> to enhance my understanding on the topic.
